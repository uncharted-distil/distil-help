---
title: "Step 5 - Check models"
sidebar: getting-started
permalink: view-models.html
previous: join-datasets
next:
---

The Check Models view lets you compare the accuracy of models generated from your feature selections. Depending on your target, Distil builds models of the following types:

- **Classification models** when your target is <a href="#" data-toggle="tooltip" data-original-title="Has a finite set of categories reused across records">categorical</a>
- **Regression models** when your target is <a href="#" data-toggle="tooltip" data-original-title="Has values that are real numbers">continuous</a>
- **Forecasting models** when you target is a <a href="#" data-toggle="tooltip" data-original-title="A compound feature consisting of a series of values recorded over time and an optional ID">timeseries</a>

Because **Event_Type** is a categorical feature, Distil generates classification models for this example. Once you have inspected them, you can either:

- Return to the previous step to refine your model definition, or
- Save the one you think is most accurate

## Review model results ##

In the Check Models view, you can inspect the models generated by Distil to: 

- Determine which one is most accurate
- Understand how specific features and values influence the results.

The Check Models view is made up of the following components:

- [Model Results pane](#model-results-pane)
- [Prediction tables](#prediction-tables)
- [Feature summaries](#feature-summaries)

{% include note.html content="The models only predict values for a subset of the actual dataset. The rest of the ground truth data is used to train the models." %}

### Model Results pane ###

The Model Results pane on the right lists each model that Distil generated for your problem. Distil ranks the models in descending order of estimated accuracy.

{% include note.html content="A progress bar under a model indicates that Distil is still generating the results." %}

<h5 class="procedure">To inspect the predictions generated by the model:</h5>

1. Scroll through the models at the bottom of the pane.
2. Compare the distribution of predictions in each model against the distribution of the actual values from the dataset. Which model looks most similar to the actual target feature?
   {% include image.html file="data-augmentation/view-models/results.png" alt="Compare model results to actual results" %}
3. Select a category in the results to split the Prediction tables based on the corresponding values.
   {% include image.html file="data-augmentation/view-models/predicted-highlight.png" alt="Highlight predicted values to view context of correct/incorrect predictions" %}
   The top table lists records that match your selection, while the bottom table lists all other records.
4. Click a different model to load its results in the [Prediction Tables](#prediction-tables).

<div class="panel-group" id="accordion">
  <div class="panel panel-default">
    <div class="panel-heading">
      <h4 class="panel-title"><a class="noCrossRef accordion-toggle" data-toggle="collapse" data-parent="#accordion" href="#collapseTwo"><span class="fa fa-plus" aria-hidden="true"></span> To adjust the acceptable error for regression models</a></h4>
    </div>
    <div id="collapseTwo" class="panel-collapse collapse noCrossRef">
      <div class="panel-body">
        Each regression model also includes the prediction error, which is calculated as the distance of the predicted value from the actual value.

        <ol>
          <li>
            <p>Review the distribution of error in the predicted results. Is the error relatively consistent across all the predictions?</p>
            {% include image.html file="model-generation/view-models/predictions-by-model-error-distribution.png" alt="Compare distribution of error across models" %}
            <p>Error charts are centered on 0. Positive error values indicate that the prediction was higher than the actual value, while negative error values indicate the opposite.</p>
          </li>
          <li>Drag the left or right error sliders to adjust the acceptable error in the predictions. By default, Distil sets the acceptable error to the 25th percentile. How does changing this value affect the number of correct/incorrect predictions?</li>
          {% include image.html file="model-generation/view-models/results-error-adjust.png" alt="Adjust acceptable error in models" %}
          {% include note.html content="Toggle between the buttons above the <strong>Error</strong> bar to adjust the range endpoints symmetrically (default) or one at a time." %}
        </ol>
      </div>
    </div>
  </div>
</div>

<h5 class="procedure">To view a different model:</h5>

1. Compare the ranges of predicted values and error in the model summaries.
2. Click the model you want to review to refresh the [Prediction Tables](#prediction-tables).

### Prediction tables ###

The Prediction tables list records that the model predicted. When you first access the Check Models page, a single Prediction table lists all the records. As you interact with feature summaries or model predictions to filter the view, the Prediction table splits into:

- **Matching** samples that contain the selected values.
- **Other** samples that do not.

You can compare the results in the two tables to understand how certain values affect the correctness of the predictions. 

{% include note.html content="In regression models, correctness is determined by the configurable error threshold." %}

<h5 class="procedure">To split the predictions tables:</h5>

- Select a category or a range of values in the target feature, feature summaries, or model predictions. What values do correct and incorrect predictions tend to have?
  {% include image.html file="data-augmentation/view-models/correct-predictions-highlight.png" alt="View values for specific records in context of the whole dataset" %}

### Feature summaries ###

The Feature summaries show the range of values in the features used to model the predictions. The distribution of values can help you understand how they influence correct/predictions. You can filter the Prediction tables on specific values to understand how results would change if you omitted them from the model.

<h5 class="procedure">To understand how the models would change if you omitted records with specific values:</h5>

- For continuous features, drag a selection on the timeline to focus on records that contain values beyond the selection. Does this improve the predictions?
  {% include image.html file="data-augmentation/view-models/feature-summaries-range-adjust.png" alt="Drag range sliders to omit outliers" %}
- For categorical features, click to select individual values. Does this improve the predictions?

## Refine a model ##

To refine the models that Distil produced, click <span class="fa fa-dot-circle-o" aria-hidden="true"></span> **New Model: Select Target** or <span class="fa fa-sign-in" aria-hidden="true"></span> **Create Models** to go back to a previous step.

{% include image.html file="data-augmentation/view-models/return-to-select-target.png" alt="Return to Select to refine the models" %}

When refining the model, you can change the target feature, add or remove features used to model the predictions, or adjust the values included for specific features.

## Finish a problem ##

When you are satisfied with the results of a model, click **Save Model** to make it available from the start page and enable to ability to apply it to make predictions on new datasets.

{% include image.html file="data-augmentation/view-models/export-model.png" alt="Export models" %}